{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### p.310 여러 번 재사용되는 텐서"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 텐서 = 벡터와 행렬의 추상적 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor(object):\n",
    "    def __init__(self, data):\n",
    "        self.data = np.array(data)\n",
    "    def __add__(self, other):\n",
    "        return Tensor(self.data + other.data)\n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "x = Tensor([1, 2, 3, 4, 5])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  4  6  8 10]\n"
     ]
    }
   ],
   "source": [
    "y = x+x\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor(object):\n",
    "    def __init__(self, data, creators=None, creation_op=None):\n",
    "        self.data = np.array(data)\n",
    "        self.creation_op = creation_op\n",
    "        self.creators = creators\n",
    "        self.grad = None\n",
    "        \n",
    "    def backward(self, grad):\n",
    "        self.grad = grad\n",
    "        if(self.creation_op == \"add\"):\n",
    "            self.creators[0].backward(grad)\n",
    "            self.creators[1].backward(grad)\n",
    "            \n",
    "    def __add__(self, other):\n",
    "        return Tensor(self.data + other.data, creators=[self, other], creation_op=\"add\")\n",
    "        \n",
    "    def __repr(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "x = Tensor([1, 2, 3, 4, 5])\n",
    "y = Tensor([2, 2, 2, 2, 2])\n",
    "\n",
    "z = x + y\n",
    "print(z)\n",
    "\n",
    "z.backward(Tensor(np.array([1, 1, 1, 1, 1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[<__main__.Tensor object at 0x000001BB6DCE4DD8>, <__main__.Tensor object at 0x000001BB6DCE4C88>]\n",
      "add\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(z.creators)\n",
    "print(z.creation_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor([1,2,3,4,5])\n",
    "b = Tensor([2,2,2,2,2])\n",
    "c = Tensor([5,4,3,2,1])\n",
    "d = Tensor([-1,-2,-3,-4,-5])\n",
    "\n",
    "e = a+b\n",
    "f = c+d\n",
    "g = e+f\n",
    "\n",
    "g.backward(Tensor(np.array([1,1,1,1,1])))\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False]\n"
     ]
    }
   ],
   "source": [
    "d = a+b\n",
    "e = b+c\n",
    "f = d+e\n",
    "\n",
    "f.backward(Tensor(np.array([1,1,1,1,1])))\n",
    "print(b.grad.data==np.array([2,2,2,2,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 6 5 4 3]\n"
     ]
    }
   ],
   "source": [
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor(object):\n",
    "    def __init__(self, data, \n",
    "                autograd=False,\n",
    "                creators=None,\n",
    "                creation_op=None,\n",
    "                id=None):\n",
    "        self.data = np.array(data)\n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.grad = None\n",
    "        self.autograd = autograd\n",
    "        self.children = {}\n",
    "        \n",
    "        if(id is None):\n",
    "            id = np.random.randint(0, 100000)\n",
    "            \n",
    "        self.id = id\n",
    "        \n",
    "        # 텐서가 얼마나 많은 자식을 가지고 있는지 계속해서 추적\n",
    "        if(creators is not None):\n",
    "            for c in creators:\n",
    "                if(self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                    \n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "                    \n",
    "                    \n",
    "    # 텐서가 각 자식으로부터 보낸 경사도 개수가 정확한지 확인                \n",
    "    def all_children_grads_accounted_for(self):\n",
    "        for id, cnt in self.children.items():\n",
    "            if(cnt != 0):\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def backward(self, grad=None, grad_origin=None):\n",
    "        if(self.autograd):\n",
    "            if(grad_origin is not None):\n",
    "                # 역전파가 가능한지 또는 경사도를 기다리고 있는 상태인지를 확인하고, 경사도를 기다리고 있다면 카운터를 감소\n",
    "                \n",
    "                if(self.children[grad_origin.id] == 0):\n",
    "                    raise Exception(\"cannot backprop more than once\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "            \n",
    "            # 여러 자식의 경사도를 누적       \n",
    "            if(self.grad is None):\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "                \n",
    "            if(self.creators is not None and (self.all_children_grads_accounted_for() or grad_origin is None)):\n",
    "                if(self.creation_op == \"add\"):\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                    # 실제 역전파 시작\n",
    "                    \n",
    "    def __add__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data, autograd = True, creators=[self, other], creation_op=\"add\")\n",
    "        \n",
    "        return Tensor(self.data + other.data)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor([1,2,3,4,5], autograd=True)\n",
    "b = Tensor([2,2,2,2,2], autograd=True)\n",
    "c = Tensor([5,4,3,2,1], autograd=True)\n",
    "\n",
    "d = a+b\n",
    "e = b+c\n",
    "f = d+e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "f.backward(Tensor(np.array([1,1,1,1,1])))\n",
    "\n",
    "print(b.grad.data == np.array([2,2,2,2,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자동 미분을 이용해서 신경망 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "target = np.array([[0], [1], [0], [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_0_1 = np.random.rand(2, 3)\n",
    "weights_1_2 = np.random.rand(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평균제곱오차:  [[0.        ]\n",
      " [0.05695952]\n",
      " [2.12810023]\n",
      " [2.88138025]]\n",
      "평균제곱오차 손실:  [5.06643999]\n",
      "총 손실값:  5.066439994622395\n",
      "평균제곱오차:  [[0.        ]\n",
      " [0.2328106 ]\n",
      " [0.26230112]\n",
      " [0.00087906]]\n",
      "평균제곱오차 손실:  [0.49599078]\n",
      "총 손실값:  0.4959907791902342\n",
      "평균제곱오차:  [[0.        ]\n",
      " [0.20128405]\n",
      " [0.21650575]\n",
      " [0.00027738]]\n",
      "평균제곱오차 손실:  [0.41806719]\n",
      "총 손실값:  0.4180671892167177\n",
      "평균제곱오차:  [[0.        ]\n",
      " [0.17018062]\n",
      " [0.18258264]\n",
      " [0.00021807]]\n",
      "평균제곱오차 손실:  [0.35298133]\n",
      "총 손실값:  0.35298133007809646\n",
      "평균제곱오차:  [[0.        ]\n",
      " [0.14186691]\n",
      " [0.15509336]\n",
      " [0.0002947 ]]\n",
      "평균제곱오차 손실:  [0.29725496]\n",
      "총 손실값:  0.2972549636567377\n",
      "평균제곱오차:  [[0.        ]\n",
      " [0.11708024]\n",
      " [0.13172121]\n",
      " [0.00043116]]\n",
      "평균제곱오차 손실:  [0.2492326]\n",
      "총 손실값:  0.2492326038163328\n",
      "평균제곱오차:  [[0.        ]\n",
      " [0.09585954]\n",
      " [0.1114102 ]\n",
      " [0.00058418]]\n",
      "평균제곱오차 손실:  [0.20785392]\n",
      "총 손실값:  0.20785392075862477\n",
      "평균제곱오차:  [[0.        ]\n",
      " [0.07795049]\n",
      " [0.09364306]\n",
      " [0.00071906]]\n",
      "평균제곱오차 손실:  [0.17231261]\n",
      "총 손실값:  0.17231260916265176\n",
      "평균제곱오차:  [[0.        ]\n",
      " [0.06299212]\n",
      " [0.07813097]\n",
      " [0.00081435]]\n",
      "평균제곱오차 손실:  [0.14193745]\n",
      "총 손실값:  0.14193744536652986\n",
      "평균제곱오차:  [[0.        ]\n",
      " [0.05060261]\n",
      " [0.06467502]\n",
      " [0.00086216]]\n",
      "평균제곱오차 손실:  [0.1161398]\n",
      "총 손실값:  0.11613979792168384\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    layer_1 = data.dot(weights_0_1)\n",
    "    layer_2 = layer_1.dot(weights_1_2)\n",
    "    \n",
    "    diff = (layer_2 - target)\n",
    "    sqdiff = (diff)**2\n",
    "    print(\"평균제곱오차: \", sqdiff)\n",
    "    loss = sqdiff.sum(0)\n",
    "    print(\"평균제곱오차 손실: \", loss)\n",
    "    \n",
    "    layer_1_grad = diff.dot(weights_1_2.T)\n",
    "    weights_1_2_update = layer_1.T.dot(diff)\n",
    "    weights_0_1_update = data.T.dot(layer_1_grad)\n",
    "    \n",
    "    weights_1_2 -= weights_1_2_update * 0.1\n",
    "    weights_0_1 -= weights_0_1_update * 0.1\n",
    "    # 0.1 = learning rate\n",
    "    \n",
    "    print(\"총 손실값: \", loss[0])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자동 추적화 추가하기\n",
    "* 확률적 경사하강법 최적화기(SGD Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "    def __inint__(self, parameters, alpha=0.1):\n",
    "        self.parametes = parameters\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def zero(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.data *= 0\n",
    "            \n",
    "    def step(self, zero=True):\n",
    "        for p in self.parameters:\n",
    "            p.data -= p.grad * self.alpha\n",
    "            \n",
    "            if(zero):\n",
    "                p.grad.data *= 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
